version: '3.8'

services:
  # Kafka broker
  kafka:
    image: confluentinc/cp-kafka:7.7.7
    #image: apache/kafka:latest
    container_name: kafka
    healthcheck:
      test: ["CMD", "bash", "-c", "echo > /dev/tcp/localhost/9092"]
      interval: 5s
      timeout: 3s
      retries: 10
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: "1@kafka:9093"
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      CLUSTER_ID: "abcdefghijklmnopqrstuv"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
    ports:
      - "9092:9092"
    networks:
      - spark-network

  # SSE to Kafka producer
  sse-producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    container_name: sse-producer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_SERVERS: kafka:9092
    networks:
      - spark-network

  consumer:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: pyspark_consumer
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      KAFKA_SERVERS: kafka:9092
    volumes:
      - ./consumer_output:/app/output
    networks:
      - spark-network

networks:
  spark-network:
    driver: bridge
  # Spark standalone
  #spark:
  #  build:
  #    context: .
  #    dockerfile: Dockerfile.spark
  #  container_name: spark
  #  ports:
  #    - "8080:8080"  # Spark UI
  #    - "4040:4040"  # Application UI
  #  depends_on:
  #    kafka:
  #      condition: service_healthy
  #  environment:
  #    SPARK_MODE: standalone
  #    KAFKA_SERVERS: kafka:9092
  #  volumes:
  #    - ./pyspark_consumer.py:/app/pyspark_consumer.py
  #    - ./output:/app/output
  #  networks:
  #    - spark-network
  #  command: >
  #    bash -c "
  #    sleep 10 &&
  #    spark-submit
  #    --master local[*]
  #    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1
  #    /app/pyspark_consumer.py
  #    --kafka-servers kafka:9092
  #    --output-path /app/output
  #    --duration 300
  #    -s -a
  #    "
